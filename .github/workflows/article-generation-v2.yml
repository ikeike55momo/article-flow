name: Article Generation Pipeline v2

on:
  workflow_dispatch:
    inputs:
      topic:
        description: '記事のトピック'
        required: true
        type: string
      store_url:
        description: '店舗URL（オプション）'
        required: false
        type: string
        default: ''
      target_audience:
        description: 'ターゲット読者'
        required: false
        type: choice
        default: 'セルフケア志向の女性'
        options:
          - 'セルフケア志向の女性'
          - '健康意識の高い男性'
          - 'シニア層'
          - '若年層'
          - 'ファミリー層'
      word_count:
        description: '目標文字数'
        required: false
        type: string
        default: '3200'
      auto_publish:
        description: 'Google Driveに自動アップロード'
        type: boolean
        default: true
      enable_image_generation:
        description: '画像生成を有効化'
        type: boolean
        default: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  RETRY_ATTEMPTS: '3'
  RETRY_WAIT_SECONDS: '30'

jobs:
  # ジョブ1: 初期化とリクエスト解析
  initialize-and-analyze:
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 10
    outputs:
      article_id: ${{ steps.init.outputs.article_id }}
      main_keyword: ${{ steps.analyze.outputs.main_keyword }}
      research_queries: ${{ steps.analyze.outputs.research_queries }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Initialize article generation
        id: init
        run: |
          ARTICLE_ID=$(date +%Y%m%d_%H%M%S)_$(echo "${{ inputs.topic }}" | tr ' ' '_' | tr -cd '[:alnum:]_')
          echo "article_id=${ARTICLE_ID}" >> $GITHUB_OUTPUT
          echo "Article ID: ${ARTICLE_ID}"
          
          # Create output directory
          mkdir -p output/${ARTICLE_ID}
          
          # Save input parameters
          cat > output/${ARTICLE_ID}/params.json << EOF
          {
            "topic": "${{ inputs.topic }}",
            "store_url": "${{ inputs.store_url }}",
            "target_audience": "${{ inputs.target_audience }}",
            "word_count": "${{ inputs.word_count }}",
            "auto_publish": ${{ inputs.auto_publish }},
            "enable_image_generation": ${{ inputs.enable_image_generation }},
            "article_id": "${ARTICLE_ID}",
            "started_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

      # Phase 1: Request Analysis using Claude Code Base Action
      - name: Phase 1 - Request Analysis
        id: analyze
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt: |
            以下の記事生成リクエストを分析してください：

            トピック: ${{ inputs.topic }}
            ターゲット読者: ${{ inputs.target_audience }}
            目標文字数: ${{ inputs.word_count }}

            以下の形式でJSONを作成し、output/${{ steps.init.outputs.article_id }}/phase1_analysis.json に保存してください：
            {
              "main_keyword": "メインキーワード",
              "sub_keywords": ["サブキーワード1", "サブキーワード2"],
              "search_intents": ["検索意図1", "検索意図2"],
              "research_queries": ["リサーチクエリ1", "リサーチクエリ2", ...],
              "content_angle": "コンテンツの切り口",
              "tone": "記事のトーン"
            }

            注意事項：
            - research_queriesは15-25個生成
            - 信頼性の高い情報源を想定したクエリ
            - 最新情報、統計、専門家意見を含む
            - 必ずJSONファイルとして保存すること
          allowed_tools: "View,Write"
          claude_env: |
            ARTICLE_ID=${{ steps.init.outputs.article_id }}
          max_turns: "3"

      - name: Verify and prepare analysis results
        run: |
          # Check if Claude created the file
          if [ -f "output/${{ steps.init.outputs.article_id }}/phase1_analysis.json" ]; then
            echo "Phase 1 analysis file found"
            cat output/${{ steps.init.outputs.article_id }}/phase1_analysis.json
          else
            echo "Phase 1 analysis file not found, creating default"
            # Create a default JSON for the next phase
            cat > output/${{ steps.init.outputs.article_id }}/phase1_analysis.json << EOF
          {
            "main_keyword": "${{ inputs.topic }}",
            "sub_keywords": [],
            "research_queries": [
              "${{ inputs.topic }} とは",
              "${{ inputs.topic }} 最新情報 2025",
              "${{ inputs.topic }} 効果 研究結果",
              "${{ inputs.topic }} 方法 手順",
              "${{ inputs.topic }} 注意点 リスク",
              "${{ inputs.topic }} メリット デメリット",
              "${{ inputs.topic }} 専門家 意見",
              "${{ inputs.topic }} 統計データ",
              "${{ inputs.topic }} 初心者向け",
              "${{ inputs.topic }} よくある質問",
              "${{ inputs.topic }} 厳生労働省",
              "${{ inputs.topic }} 健康被害",
              "${{ inputs.topic }} 治療法",
              "${{ inputs.topic }} 予防法",
              "${{ inputs.topic }} 成功事例"
            ],
            "content_angle": "総合ガイド",
            "tone": "親しみやすく専門的"
          }
          EOF
          fi
          
          # Extract for output
          MAIN_KEYWORD=$(jq -r '.main_keyword' output/${{ steps.init.outputs.article_id }}/phase1_analysis.json)
          RESEARCH_QUERIES=$(jq -c '.research_queries' output/${{ steps.init.outputs.article_id }}/phase1_analysis.json)
          
          echo "main_keyword=${MAIN_KEYWORD}" >> $GITHUB_OUTPUT
          echo "research_queries=${RESEARCH_QUERIES}" >> $GITHUB_OUTPUT

      - name: Upload phase artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase1-${{ steps.init.outputs.article_id }}
          path: output/${{ steps.init.outputs.article_id }}
          retention-days: 30

  # ジョブ2: 並列Web検索（5バッチでGemini API使用）
  research:
    needs: initialize-and-analyze
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 20
    strategy:
      matrix:
        batch: [0, 1, 2, 3, 4]
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download previous artifacts
        uses: actions/download-artifact@v4
        with:
          name: phase1-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Gemini dependencies
        run: |
          pip install google-genai

      # Phase 2: Parallel research using Gemini API
      - name: Phase 2 - Parallel Research with Gemini (Batch ${{ matrix.batch }})
        id: research
        run: |
          # Create research script for batch processing
          cat > research_batch_gemini.py << 'EOF'
          import os
          import sys
          import json
          import google.generativeai as genai
          from datetime import datetime

          # Configure Gemini
          genai.configure(api_key=os.environ['GEMINI_API_KEY'])
          model = genai.GenerativeModel('gemini-2.0-flash-exp')

          # Get batch parameters
          batch_num = int(sys.argv[3])
          total_batches = 5

          # Read analysis results with error handling
          try:
              with open(sys.argv[1], 'r', encoding='utf-8') as f:
                  analysis = json.load(f)
          except json.JSONDecodeError as e:
              print(f"Error parsing JSON: {e}")
              print("Using default queries")
              analysis = {
                  "research_queries": [
                      "${{ inputs.topic }} とは",
                      "${{ inputs.topic }} 最新情報 2025",
                      "${{ inputs.topic }} 効果 研究結果",
                      "${{ inputs.topic }} 方法 手順",
                      "${{ inputs.topic }} 注意点 リスク",
                      "${{ inputs.topic }} メリット デメリット",
                      "${{ inputs.topic }} 専門家 意見",
                      "${{ inputs.topic }} 統計データ",
                      "${{ inputs.topic }} 初心者向け",
                      "${{ inputs.topic }} よくある質問",
                      "${{ inputs.topic }} 厚生労働省",
                      "${{ inputs.topic }} 健康被害",
                      "${{ inputs.topic }} 治療法",
                      "${{ inputs.topic }} 予防法",
                      "${{ inputs.topic }} 成功事例",
                      "${{ inputs.topic }} 失敗事例",
                      "${{ inputs.topic }} 副作用",
                      "${{ inputs.topic }} 使用方法",
                      "${{ inputs.topic }} 価格 費用",
                      "${{ inputs.topic }} 比較 違い"
                  ]
              }

          all_queries = analysis.get('research_queries', [])
          
          # Calculate batch slice
          queries_per_batch = len(all_queries) // total_batches + (1 if len(all_queries) % total_batches > 0 else 0)
          start_idx = batch_num * queries_per_batch
          end_idx = min(start_idx + queries_per_batch, len(all_queries))
          batch_queries = all_queries[start_idx:end_idx]

          print(f"Batch {batch_num}: Processing queries {start_idx}-{end_idx-1} ({len(batch_queries)} queries)")
          
          results = []

          for i, query in enumerate(batch_queries):
              print(f"Batch {batch_num} - Searching ({i+1}/{len(batch_queries)}): {query}")
              
              prompt = f"""
              Web検索を実行: "{query}"
              
              重要な指示：
              - 実際のウェブサイトのURL（ドメイン名を含む）を提供してください
              - リダイレクトURLではなく、元のソースのURLを記載してください
              - 一次情報（政府機関、学術機関）を最優先してください
              
              優先順位：
              1. 政府機関（.go.jp, .gov） - 厚生労働省、消費者庁、政府データベース等
              2. 学術機関（.ac.jp, .edu） - 大学、研究機関、学術論文等
              3. 医学会・専門団体 - 日本皮膚科学会、医師会等
              4. 大手メディア - 信頼できる健康メディア
              
              以下の形式でJSONで返してください：
              {{
                "query": "{query}",
                "results": [
                  {{
                    "url": "実際のウェブサイトURL（例: https://www.mhlw.go.jp/...）",
                    "domain": "ドメイン名（例: mhlw.go.jp）",
                    "title": "タイトル",
                    "source_type": "government/academic/medical/industry/media",
                    "reliability_score": 1-10,
                    "key_findings": ["重要な発見、具体的なデータや統計を含む"],
                    "publication_date": "YYYY-MM-DD",
                    "author": "著者名や組織名（わかる場合）"
                  }}
                ]
              }}
              
              注意：
              - vertexaisearch.cloud.google.comのリダイレクトURLは使用しないでください
              - 実際の情報源のURLを提供してください
              - 政府・学術機関の情報を優先的に検索してください
              """
              
              try:
                  response = model.generate_content(
                      prompt,
                      tools=['google_search'],
                      generation_config=genai.GenerationConfig(
                          temperature=1.0,
                          max_output_tokens=2048
                      )
                  )
                  
                  # Parse response
                  text = response.text
                  json_start = text.find('{')
                  json_end = text.rfind('}') + 1
                  if json_start >= 0 and json_end > json_start:
                      result = json.loads(text[json_start:json_end])
                      results.append(result)
                  
              except Exception as e:
                  print(f"Error for query '{query}': {e}")
                  results.append({"query": query, "results": [], "error": str(e)})

          # Save batch results
          output = {
              "batch_num": batch_num,
              "queries_processed": batch_queries,
              "search_results": results,
              "completed_at": datetime.utcnow().isoformat(),
              "total_results": sum(len(r.get("results", [])) for r in results)
          }

          with open(sys.argv[2], 'w', encoding='utf-8') as f:
              json.dump(output, f, ensure_ascii=False, indent=2)

          print(f"Batch {batch_num} completed: {len([r for r in results if r.get('results')])} successful queries")
          EOF
          
          # Run batch research
          python research_batch_gemini.py \
            output/${{ needs.initialize-and-analyze.outputs.article_id }}/phase1_analysis.json \
            output/${{ needs.initialize-and-analyze.outputs.article_id }}/phase2_research_batch_${{ matrix.batch }}.json \
            ${{ matrix.batch }}
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}

      - name: Upload batch research artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase2-batch-${{ matrix.batch }}-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}/phase2_research_batch_${{ matrix.batch }}.json
          retention-days: 30

  # ジョブ2.5: リサーチ結果統合
  research-merge:
    needs: [initialize-and-analyze, research]
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all batch artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phase2-batch-*-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: batch_results/
          merge-multiple: true

      - name: Download phase1 artifacts
        uses: actions/download-artifact@v4
        with:
          name: phase1-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}

      - name: Merge batch results
        run: |
          python3 << 'EOF'
          import json
          import glob
          import os
          from datetime import datetime
          
          # Find all batch result files
          batch_files = glob.glob('batch_results/phase2_research_batch_*.json')
          batch_files.sort()
          
          print(f"Found {len(batch_files)} batch files:")
          for f in batch_files:
              print(f"  - {f}")
          
          merged_results = []
          total_queries = []
          
          for batch_file in batch_files:
              try:
                  with open(batch_file, 'r', encoding='utf-8') as f:
                      batch_data = json.load(f)
                  
                  batch_num = batch_data.get('batch_num', 0)
                  queries = batch_data.get('queries_processed', [])
                  results = batch_data.get('search_results', [])
                  
                  print(f"Batch {batch_num}: {len(queries)} queries, {len(results)} results")
                  
                  total_queries.extend(queries)
                  merged_results.extend(results)
                  
              except Exception as e:
                  print(f"Error processing {batch_file}: {e}")
          
          # Create merged output
          merged_output = {
              "research_queries": total_queries,
              "search_results": merged_results,
              "completed_at": datetime.utcnow().isoformat(),
              "total_results": sum(len(r.get("results", [])) for r in merged_results),
              "batches_processed": len(batch_files)
          }
          
          # Ensure output directory exists
          os.makedirs('output/${{ needs.initialize-and-analyze.outputs.article_id }}', exist_ok=True)
          
          # Save merged results
          with open('output/${{ needs.initialize-and-analyze.outputs.article_id }}/phase2_research.json', 'w', encoding='utf-8') as f:
              json.dump(merged_output, f, ensure_ascii=False, indent=2)
          
          print(f"Merged research completed:")
          print(f"  - Total queries: {len(total_queries)}")
          print(f"  - Total results: {merged_output['total_results']}")
          print(f"  - Batches processed: {len(batch_files)}")
          EOF

      - name: Upload merged research artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase2-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          retention-days: 30

  # ジョブ3-4: 構成計画と執筆
  structure-and-write:
    needs: [initialize-and-analyze, research-merge]
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 25
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phase*-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          merge-multiple: true

      # Phase 3: Structure Planning
      - name: Phase 3 - Structure Planning
        id: structure
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt_file: prompts/03_structure.md
          allowed_tools: "View,Write"
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
            TOPIC=${{ inputs.topic }}
            TARGET_AUDIENCE=${{ inputs.target_audience }}
            WORD_COUNT=${{ inputs.word_count }}
          max_turns: "5"

      # Phase 4: Writing
      - name: Phase 4 - Writing
        id: writing
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt_file: prompts/04_writing.md
          allowed_tools: "View,Write,Edit"
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
            TOPIC=${{ inputs.topic }}
            WORD_COUNT=${{ inputs.word_count }}
          max_turns: "10"

      - name: Upload writing artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase4-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          retention-days: 30

  # ジョブ5-6: ファクトチェックとSEO最適化（並列実行）
  factcheck:
    needs: [initialize-and-analyze, structure-and-write]
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phase*-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          merge-multiple: true

      # Phase 5: Fact Check
      - name: Phase 5 - Fact Check
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt_file: prompts/05_factcheck.md
          allowed_tools: "View,Edit"
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
          max_turns: "5"

      - name: Upload factcheck artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase5-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          retention-days: 30

  seo-optimization:
    needs: [initialize-and-analyze, structure-and-write]
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phase*-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          merge-multiple: true

      # Phase 6: SEO Optimization
      - name: Phase 6 - SEO Optimization
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt_file: prompts/06_seo.md
          allowed_tools: "View,Write,Edit"
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
            MAIN_KEYWORD=${{ needs.initialize-and-analyze.outputs.main_keyword }}
          max_turns: "8"

      - name: Upload SEO artifacts
        uses: actions/upload-artifact@v4
        with:
          name: phase6-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          retention-days: 30

  # ジョブ7: 画像生成（MCP + Imagen4経由）
  generate-images:
    if: ${{ inputs.enable_image_generation }}
    needs: [initialize-and-analyze, structure-and-write]
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phase*-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          merge-multiple: true

      # NEW: Use Claude Code with MCP for image generation
      - name: Generate Images with Claude + MCP Imagen4
        id: generate
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt: |
            記事の画像を生成してください。
            
            記事ディレクトリ: .
            
            タスク:
            1. 02_article_structure.md を読み込んで記事構造を理解
            2. 04_optimized_draft.html を読み込んで記事内容を把握
            3. 以下の画像を生成:
               - ヒーロー画像（16:9）: 記事全体のテーマを表現
               - 各主要セクション用画像（4:3）: 最大4枚
            4. 各画像について:
               - プロンプトを日本の健康・美容記事に最適化
               - プロフェッショナルで信頼感のあるスタイル
               - テキストやロゴを含まない
            5. 生成した画像を images/ に保存
            6. images/metadata.json を作成して画像情報を記録
            
            重要: 健康・美容分野の薬機法・景表法に配慮した適切な表現を使用
          
          # MCPサーバー設定 - Gemini API経由でImagen4アクセス
          mcp_config: |
            {
              "mcpServers": {
                "gemini-imagen": {
                  "command": "npx",
                  "args": [
                    "-y", 
                    "gemini-imagen-mcp-server",
                    "--model", "imagen-4"
                  ],
                  "env": {
                    "GEMINI_API_KEY": "${{ secrets.GEMINI_API_KEY }}"
                  }
                }
              }
            }
          
          # 使用可能なツール
          allowed_tools: |
            View,
            Write,
            Edit,
            mcp__gemini-imagen__generate_image,
            mcp__gemini-imagen__list_models
          
          # 環境変数
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
          
          # 最大実行回数
          max_turns: "15"

      # Verify images were generated
      - name: Verify and list generated images
        run: |
          IMAGE_DIR="output/${{ needs.initialize-and-analyze.outputs.article_id }}/images"
          
          # MCPサーバーは imagen/ ディレクトリに画像を保存する
          if [ -d "imagen" ] && [ "$(ls -A imagen 2>/dev/null)" ]; then
            echo "✅ Images found in imagen directory:"
            ls -la imagen/
            
            # 画像を正しいディレクトリにコピー
            mkdir -p $IMAGE_DIR
            cp imagen/*.png $IMAGE_DIR/ 2>/dev/null || true
            cp imagen/*.jpg $IMAGE_DIR/ 2>/dev/null || true
            cp imagen/*.webp $IMAGE_DIR/ 2>/dev/null || true
            
            echo "Copied to $IMAGE_DIR:"
            ls -la $IMAGE_DIR
            echo "Image count: $(find $IMAGE_DIR -name "*.png" -o -name "*.jpg" -o -name "*.webp" | wc -l)"
          elif [ -d "$IMAGE_DIR" ] && [ "$(ls -A $IMAGE_DIR 2>/dev/null)" ]; then
            echo "✅ Images already in correct directory:"
            ls -la $IMAGE_DIR
            echo "Image count: $(find $IMAGE_DIR -name "*.png" -o -name "*.jpg" -o -name "*.webp" | wc -l)"
          else
            echo "⚠️ No images generated, creating placeholder directory..."
            mkdir -p $IMAGE_DIR
            echo '{"error": "Image generation failed", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > $IMAGE_DIR/metadata.json
          fi

      - name: Upload image artifacts
        uses: actions/upload-artifact@v4
        with:
          name: images-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}/images
          retention-days: 30

  # ジョブ8: 最終調整とアップロード
  finalize-and-upload:
    needs: [initialize-and-analyze, factcheck, seo-optimization, generate-images]
    if: always()
    runs-on: ubuntu-latest
    environment: GA
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '*-${{ needs.initialize-and-analyze.outputs.article_id }}'
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          merge-multiple: true

      # Phase 7: Final Adjustment
      - name: Phase 7 - Final Adjustment
        uses: anthropics/claude-code-base-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt_file: prompts/07_final.md
          allowed_tools: "View,Edit,Write"
          claude_env: |
            ARTICLE_ID=${{ needs.initialize-and-analyze.outputs.article_id }}
          max_turns: "5"

      - name: Setup Python for upload
        if: ${{ inputs.auto_publish && success() }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install upload dependencies
        if: ${{ inputs.auto_publish && success() }}
        run: pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pyyaml

      # Upload to Google Drive
      - name: Upload to Google Drive
        if: ${{ inputs.auto_publish && success() }}
        run: |
          python github-actions/scripts/upload_to_drive.py \
            --article-dir output/${{ needs.initialize-and-analyze.outputs.article_id }} \
            --drive-folder-id ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        env:
          GOOGLE_DRIVE_CREDENTIALS: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS }}

      # Generate final report
      - name: Generate Final Report
        run: |
          cat > output/${{ needs.initialize-and-analyze.outputs.article_id }}/final_report.json << EOF
          {
            "article_id": "${{ needs.initialize-and-analyze.outputs.article_id }}",
            "topic": "${{ inputs.topic }}",
            "status": "${{ job.status }}",
            "completed_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "phases_completed": {
              "analysis": true,
              "research": true,
              "structure": true,
              "writing": true,
              "factcheck": ${{ needs.factcheck.result == 'success' }},
              "seo": ${{ needs.seo-optimization.result == 'success' }},
              "images": ${{ needs.generate-images.result == 'success' }}
            }
          }
          EOF

      - name: Upload final artifacts
        uses: actions/upload-artifact@v4
        with:
          name: final-${{ needs.initialize-and-analyze.outputs.article_id }}
          path: output/${{ needs.initialize-and-analyze.outputs.article_id }}
          retention-days: 30

      # Send notification
      - name: Send Slack Notification
        if: ${{ env.SLACK_WEBHOOK != '' }}
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        run: |
          STATUS="${{ job.status }}"
          COLOR="good"
          if [ "$STATUS" != "success" ]; then COLOR="danger"; fi
          
          curl -X POST $SLACK_WEBHOOK \
            -H 'Content-type: application/json' \
            -d '{
              "attachments": [{
                "color": "'$COLOR'",
                "title": "Article Generation Complete",
                "fields": [
                  {"title": "Topic", "value": "${{ inputs.topic }}", "short": true},
                  {"title": "Status", "value": "'$STATUS'", "short": true},
                  {"title": "Article ID", "value": "${{ needs.initialize-and-analyze.outputs.article_id }}", "short": false}
                ]
              }]
            }'